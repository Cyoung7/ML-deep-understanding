# Decision Tree

[TOC]

### 思想概述

​	决策树是一个预测模型,他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，而每个叶结点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。

​	核心思想:算法通过不断划分数据集来生成决策树，其中每一步的划分能够使当前的信息增益(比)达到最大.

该核心思想的背后其实也有着机器学习的一些普适性的思想.可以这样来看待决策树：模型的损失就是数据集的不确定性，模型的算法就是最小化该不确定性；同时，和许多其它模型一样，想要从整个参数空间中选出模型的最优参数是个 $NP$完全问题，所以我们（和许多其它算法一样）采用启发式的方法、近似求解这个最优化问题。具体而言，我们每次会选取一个局部最优解（每次选取一个特征对数据集进行划分使得信息增益(比)最大化）、并把这些局部解合成最终解（合成一个划分规则的序列）

## 信息论知识

###信息不确定性

​	在决策树的生成中，获得的信息量的度量方法是从反方向来定义的：若一种划分能使数据的“不确定性”减少得越多、就意味着该划分能获得越多信息。这是很符合直观的，关键问题就在于应该如何度量数据的不确定性（或说不纯度，Impurity).

常见的度量标准有两个：信息熵（Entropy）和基尼系数（Gini Index），接下来我们就说说它们的定义和性质.

###信息熵

​	公式:

​						$H(y)=−\sum_{k=1}^Kp_klog(p_k)$								(1)

​	对于具体的、随机变量y生成的数据集 $D=\{y_1,…,y_N\}$ 而言，一般使用经验熵来估计真正的信息熵(**使用频率来估计概率**,频率估计理论见极大似然估计):

​						$H(y)=H(D)=−\sum_{k=1}^K\frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}$						(2)	

### 基尼系数

​	公式:

​						$Gini(y)=\sum_{k=1}^Kp_k(1−p_k)=1−\sum_{k=1}^Kp^2_k$				(2)

​	对于具体的、随机变量y生成的数据集 $D=\{y_1,…,y_N\}$ 而言，一般使用经验基尼来估计真正的基尼系数(**使用频率来估计概率**):

​						$Gini(y)=Gini(D)=1−\sum_{k=1}^K(\frac{|C_k|}{|D|})^2$

​	同样可以证明,当:

​								$p_1=p_2=…=p_K=\frac1K$

​	时, $Gini(y)$ 取得最大值 $1−\frac1K$ ；当存在 $k^∗$使得 $p_{k^∗}=1$ 时, $Gini(y)=0$ 。特别地,当 $K=2$ 时，可以导出：

​								$Gini(y)=2p(1−p)$

​						

### 信息增益

​	在定义完不确定性的度量标准之后,我们就可以看看什么叫“获得信息”,亦即信息的增益了. 从直观上来说,信息的增益是针对随机变量 $y$ 和描述该变量的特征来定义的.此时数据集 $D=\{(x_1,y_1),…,(x_N,y_N)\}$ ,其中 $x_i=(x^{(1)}_i,…,x^{(n)}_i)^T$ 是描述 $y_i$ 的特征向量, $n$ 则是特征个数。我们可以先研究单一特征的情况 $(n=1)$：不妨设该特征叫 $A$、数据集 $D=\{(A_1,y_1),…,(A_N,y_N)\}$ ;此时所谓信息的增益，反映的就是特征 $A$ 所能给我们带来的关于 $y$ 的“信息量”的大小.

可以引入条件熵  $H(y|A)$ 的概念来定义信息的增益,同样比较直观:

- 所谓条件熵，就是根据特征 $A$ 的不同取值 $\{a_1,…,a_m\}$  对 $y$ 进行限制后，先对这些被限制的 $y$ 分别计算信息熵、再把这些信息熵（一共有 $m$ 个）根据特征取值本身的概率加权求和、从而得到总的条件熵。换句话说，条件熵是由被A不同取值限制的各个部分的y的不确定性以取值本身的概率作为权重加总得到的.

- 数学定义:

  ​					$H(y|A)=\sum_{j=1}^mp(A=a_j)H(y|A=a_j)$

  ​	其中:

  ​		$p(A=a_j)$:取值 $a_j$ 在特征 $A$  的概率

  ​		$H(y|A=a_j)=−\sum_{k=1}^Kp(y=c_k|A=a_j) \log\ p(y=c_k|A=a_j)$

  ​

  同样可以用经验条件熵来估计真正的条件熵：

  ​					$H(y|A)=H(y|D)=\sum_{j=1}^m\frac{|D_j|}{|D|}\sum_{k=1}^K\frac{|D_{jk}|}{|Dj|}log\frac{|D_{jk}|}{|D_j|}$

  这里的 $D_j$ 表示在 $A=a_j$ 限制下的数据集, $|D_{jk}|$ 则代表着 $D_j$中第 $k$ 类样本的个数.

  根据上面分别介绍的信息熵和条件熵,信息增益的定义如下:

  ​							$g(y,A)=H(y)−H(y|A)$

  这里的 $g(y,A)$ 常被称为互信息.决策树中的 <u>**ID3 算法**</u> 即是利用信息增益(互信息)来作为特征选取的标准.

但是，如果简单地以 $g(y,A)$ 作为标准的话，会存在偏向于选择取值较多的特征、也就是 $m$  比较大的特征的问题。我们仍然可以从直观上去理解为什么会偏向于选取 $m$ 较大的特征以及为什么这样做是不尽合理的：

- 我们希望得到的决策树应该是比较深（又不会太深）的决策树，从而它可以基于多个方面而不是片面地根据某些特征来判断
- 如果单纯以 $g(y,A)$ 作为标准，由于 $g(y,A)$ 的直观意义是 $y$ 被 $A$ 划分后不确定性的减少量，可想而知,<u>**当A的取值很多时,y会被A划分成很多份,于是其不确定性自然会减少很多**</u>,从而 ID3 算法会倾向于选择A作为划分依据。但如果这样做的话，可以想象、我们最终得到的决策树将会是一颗很胖很矮的决策树，这并不是我们想要的.

为解决该问题、我们可以给 m 一个惩罚，由此我们可以得到信息增益比的概念.

<u>**需要指出的是，只需要类比上述的过程、我们同样可以使用基尼系数来定义信息增益.**</u>

​	先定义条件基尼系数:

​						$Gini(y|A)=\sum_{j=1}^mp(A=a_j)Gini(y|A=a_j)$

​	其中:

​		$Gini(y|A=a_j)=1−\sum_{k=1}^Kp^2(y=c_k|A=a_j)$	

​	同样可以用经验条件基尼系数来进行估计(**使用频率来估计概率**)：

​		$Gini(y|A)=Gini(y|D)=\sum_{j=1}^m\frac{|D_j|}{|D|}[1−\sum_{k=1}^{K}(\frac{|D_{jk}|}{|D_j|})^2]=1−\sum_{j=1}^m\frac{|D_j|}{|D|}\sum_{k=1}^K(\frac{|D_{jk}|}{|D_j|})^2$

​	信息的增益则自然地定义为（不妨称之为“基尼增益”）：

​						$g_{Gini}(y,A)=Gini(y)−Gini(y|A)$

​	决策树算法中的 <u>**CART算法**</u> 通常会应用这种定义.

###信息增益比

​	定义:

​						$g_R(y,A)=\frac{g(y,A)}{H(A)}$

​	其中 $H(A)$ 是特征 $A$ 的熵,它的定义:

​						$H(A)=−\sum_{j=1}^mp(A=a_j) \log p(A=a_j)$	

​	同样可以用经验熵来进行估计(**使用频率来估计概率**):

​						$H(A)=H_A(D)=−\sum_{j=1}^m\frac{|D_j|}{|D|}\log\frac{|D_j|}{|D|}$

​	该定义式和信息熵的定义式很像，它们的性质也有相通之处.

决策树中的 <u>**ID4.5 算法**</u> 即是利用信息增益比($g_R(y,A)$)来作为特征选取的标准



## 决策树的生成算法

​	决策树的生成算法发展至今已经有许多变种，想要全面介绍它们不是短短一篇文章所能做到的。本文拟介绍其中三个上一节有所提及的、相对而言比较基本的算法：ID3、C4.5 和 CART。它们本身存在着某种递进关系：

- ID3 算法是“最朴素”的决策树算法，它给出了对离散型数据分类的解决方案
- C4.5 算法在其上进一步发展、给出了对混合型数据分类的解决方案
- CART 算法则更进一步、给出了对数据回归的解决方案



可以直观叙述一颗决策树的生成过程:

- 向根节点输入数据

- 根据信息<u>**增益的度量**</u>、选择数据的某个特征来把数据划分成（**互不相交的**）好几份并分别喂给一个新 Node

- 如果分完数据后发现：

  - 某份数据的不确定较小、亦即其中某一类别的样本已经占了大多数，此时就不再对这份数据继续进行划分、将对应的 Node 转化为叶节点

  - 某份数据的不确定性仍然较大，那么这份数据就要继续分割下去（转第 2 步）



### ID3（Interactive Dichotomizer-3）

ID3 可以译为“交互式二分法”，虽说这个名字里面带了个“二分”，但该方法完全适用于“多分”的情况。它选择信息增益(互信息)作为信息增益的度量、针对离散型数据进行划分。其算法叙述如下：

​	1.**输入**：训练数据集 $D={(x_1,y_1),…,(x_N,y_N)}$

​	2.**过程**：

​		1.将数据集D喂给一个$Node$

​		2.若 $D$ 中的所有样本同属于类别 $c_k$ ，则该 $Node$ 不再继续生成、并将其类别标记为 $c_k$ 类

​		3.若$x_i$ 已经是 $0$ 维向量、亦即已没有可选特征，则将此时 $D$ 中样本个数最多的类别 $c_k$ 作为该 $Node$ 的类别

​		4.否则，按照互信息定义的信息增益：

​							  	$g(y,x^{(j)})=H(y)−H(y|x^{(j)})$

​		计算第 $j$ 维特征的信息增益,然后选择使得信息增益最大的特征 $x^{j^*}$ 作为划分标准,亦即:

​								$j^{*}=\arg\max_jg(y,x^j)$

​		5.若 $x^(j^*)$ 满足停止条件、则不再继续生成并则将此时 $D$ 中样本个数最多的类别 $c_k$ 作为类别标记

​		6.否则，依 $x^{(j^*)}$ 的所有可能取值 $\{a_1,…,a_m\}$ 将数据集 $D$ 划分为 $\{D_1,…,D_m\}$ ,使得：

  								$(xi,yi)∈D_j⇔x^{(j^*)}_i=a_j,∀i=1,…,N$

​		    同时，将 $x_1,…,x_N$ 的第 $j^*$ 维去掉、使它们成为 $n−1$ 维的特征向量

​		7.对每个$D_j$ 从 2.1 开始调用算法

​	3.**输出**：原始数据对应的 $Node$（亦即根节点）

其中算法第 2.5 步的“停止条件”（也可称为"预剪枝",下一节详细讲解)，常用的是如下两种：

- 若选择 $x^{(j^*)}$ 作为特征时信息增益 $g(y,x^{(j^*)})$ 仍然很小（通常会传入一个参数$ϵ$作为阈值）、则停止
- 事先把数据集分为训练集与测试集（交叉验证的思想），若由训练集得到的 $x^{(j^*)}$ 并不能使得决策树在测试集上的错误率更小、则停止

### C4.5

C4.5 使用**信息增益比**作为信息增益的度量，从而缓解了 ID3 算法会倾向于选择 m 比较大的特征A作为划分依据这个问题；也正因如此，C4.5 算法可以处理 ID3 算法比较难处理的混合型数据。我们先来看看它在离散型数据上的算法（仅展示和 ID3 算法中不同的部分）

**算法 2.4 步**

​	按照信息增益比定义的信息增益:

​							$g_R(y,x^{(j)})=\frac{g(y,x^{(j)})}{H_{x^{(j)}}(y)}$

​	来计算第 $j$ 维特征的信息增益，然后选择使得信息增益最大的特征 $x^{(j^*)}$作为划分标准，亦即：

​							$j^*=\arg\max_jg_R(y,x^{(j)})$

**注意：C4.5 算法虽然不会倾向于选择 m 比较大的特征、但有可能会倾向于选择 m 比较小的特征。针对这个问题，Quinlan 在 1993 年提出了这么一个启发式的方法：先选出互信息比平均互信息要高的特征、然后从这些特征中选出信息增益比最高的**



###CART

CART 的全称是 Classification and Regression Tree（“分类与回归树”).顾名思义,它既可做分类亦可做回归.

CART 算法一般会使用基尼增益作为信息增益的度量（当然也可以使用互信息和信息增益比作为度量，需要视具体场合而定），其一大特色就是它假设了最终生成的决策树为二叉树、亦即它在处理离散型特征时也会通过决出二分标准来划分数据:

**算法 2.4 步**

否则，不妨设 $x^{(j)}$ 在当前数据集中有 $S_j$ 个取值 $u^{(j)}_1,…,u^{(j)}_{S_j}$ 且它们满足 $u^{(j)}_1<…<u^{(j)}_{S_j}$ ，则：

- 若 $x^{(j)}$是离散型的，则依次选取 $u^{(j)}_1,…,u^{(j)}_{S_j}$作为二分标准$a_p$，此时：

  ​						  $A_{jp}=\{x^{(j)}=a_p,x^{(j)}≠a_p\}$

- 若 $x^{(j)}$是连续型的，则依次选取$\frac{u^{(j)}_1+u^{(j)}_2}2,…,\frac{u^{(j)}_{S_j-1}+u^{(j)}_{S_j}}2$作为二分标准 $a_p$，此时：

  ​						$A_{jp}=\{x^{(j)}<a_p,x^{(j)}≥a_p\}$

  按照基尼系数定义的信息增益：  

  ​						$g_{Gini}(y,A_{jp})=Gini(y)−Gini(y|A_{jp})$

  来计算第 $j$ 维特征在这些二分标准下的信息增益，然后选择使得信息增益最大的特征 $x^{(j^*)}$和相应的二分标准 $u^{(j^*)}_{p^*}$ 作为划分标准，亦即：  

  ​						$(j^*,p^*)=\arg\max_{j,p}g_{Gini}(y,A_{jp})$

从分类问题到回归问题不是一个不平凡的问题，它们的区别仅在于：回归问题除了特征是连续型的以外、“类别”也是连续型的，此时我们一般把“类别向量”改称为“输出向量”。正如前文所提及，决策树可以转化为最小化损失的问题。我们之前讨论的分类问题中的损失都是数据的不确定性，而在回归问题中、一种常见的做法就是将损失定义为平方损失:

​							$L(D)=\sum_{i=1}^NI(y_i≠f(x_i))[y_i−f(x_i)]^2$

这里的 $I$是示性函数，f是我们的模型、$f(x_i)$ 是 $x_i$ 在我们模型下的预测输出、$y_i$ 是真实输出。平方损失其实就是我们熟悉的“（欧式）距离”（预测向量和输出向量之间的距离），我们会在许多分类、回归问题中见到它的身影。在损失为平方损失时，一般称此时生成的回归决策树为最小二乘回归树



## 决策树的剪枝算法



