# NaiveBayes

[TOC]



## 综述

朴素贝叶斯（Naive Bayes）是贝叶斯分类器的一种，而后者是一个相当宽泛的定义，它背后的数学理论根基是相当出名的贝叶斯决策论（Bayesian Decision Theory）。贝叶斯决策论和传统的统计学理论有着区别，其中最不可调和的就是它们各自关于概率的定义。因此，使用了贝叶斯决策论作为基石的贝叶斯分类器，在各个机器学习算法所导出的分类器中也算是比较标新立异的存在

## 1.贝叶斯决策论

贝叶斯决策论是在概率框架下进行决策的基本方法之一、更是统计模式识别的主要方法之一！

名字也许能看出来，贝叶斯决策论其实是贝叶斯统计学派进行决策的方法。为了更加深刻地理解贝叶斯分类器，我们需要先对贝叶斯学派和其决策理论有一个大致的认知。

### 贝叶斯学派与频率学派

贝叶斯学派强调概率的“主观性”，这一点和传统的、我们可能比较熟悉的频率学派不同。详细的论述牵扯到许多概率论和数理统计的知识，这里只说一个直观：

- 频率学派强调频率的“自然属性”，认为应该使用事件在重复试验中发生的频率作为其发生的概率的估计
- 贝叶斯学派不强调事件的“客观随机性”，认为仅仅只是“观察者”不知道事件的结果。换句话说，贝叶斯学派认为：事件之所以具有随机性仅仅是因为“观察者”的知识不完备，对于“知情者”来说、该事件其实不具备随机性。随机性的根源不在于事件，而在于“观察者”对该事件的知识状态

举个栗子：假设一个人抛了一枚均匀硬币到地上并迅速将其踩在脚底而在他面前从近到远坐了三个人。他本人看到了硬币是正面朝上的，而其他三个人也多多少少看到了一些信息，但显然坐得越远、看得就越模糊。频率学派会认为，该硬币是正是反、各自的概率都应该是 50%；但是贝叶斯学派会认为，对抛硬币的人来说、硬币是正面的概率就是 100%，然后可能对离他最近的人来说是 80%、对离他最远的人来说就可能还是 50%

所以相比起**把模型参数固定、注重样本的随机性的频率学派**而言，**贝叶斯学派将样本视为是固定的、把模型的参数**视为关键

在上面这个例子里面，样本就是抛出去的那枚硬币，模型的参数就是每个人从中获得的“信息”。对于频率学派而言，每个人获得的“信息”不应该有不同，所以自然会根据“均匀硬币抛出正面的概率是 50%”这个“样本的信息”来导出“硬币是正面的概率为 50%”这个结论。但是对贝叶斯学派而言，硬币抛出去就抛出去了，问题的关键在于模型的参数、亦即“观察者”从中获得的信息，所以会导出“对于抛硬币的人而言，硬币是正面的概率是 100%”这一类的结论

### 贝叶斯决策论

大致知道贝叶斯学派的思想后，我们就可以介绍贝叶斯决策论了。这里不可避免地要牵扯到概率论和数理统计的相关定义和知识，但幸运的是它们都是比较基础且直观的部分、无需太多数学背景就可以知道它们的含义：
**行动空间**

行动空间（通常用$A$来表示）是某项实际工作中可能采取的各种“行动”所构成的集合。正如前文所提到的、贝叶斯学派注重的是模型参数，所以通常而言我们想要做出的“行动”是“决定模型的参数”。因此我们通常会将行动空间取为参数空间，亦即$A=\Theta$

**决策**

决策（通常用 $\delta(\breve{X})$ 来表示）是样本空间 $X$ 到行动空间 $A$ 的一个映射。换句话说，对于一个单一的样本 $\breve{X} (\breve{X}∈X)$，决策函数可以利用它得到A中的一个行动。需要注意的是，这里的样本X~通常是高维的随机向量：$\breve{X}=(x_1,...,x_N)^T$；尤其需要分清的是，这个（以及本节之后的所有）$\breve{X}$其实是一般意义上的“训练集”、$x_i$才是一般意义上的“样本”。这是因为本节主要在叙述数理统计相关知识，所以在术语上和机器学习术语会有所冲突，需要分辨清它们的关系

**损失函数**

损失函数（通常用$L(θ,a)=L(θ,δ(\breve{X}))$来表示）用于衡量当参数是$θ（θ∈Θ)$，$Θ$是参数空间）时采取行动$a(a∈A)$所引起的损失

**决策风险**

决策风险（通常用$R(θ,δ)$来表示）是损失函数的期望：$R(θ,δ)=EL(θ,δ(X~))$

**先验分布**

先验分布描述了参数$θ$在已知样本$\breve{X}$中的分布

**平均风险**

平均风险（通常用$ρ(δ)$来表示）定义为决策风险$R(θ,δ)$在先验分布下的期望：

​									$ρ(δ)=E_ξR(θ,δ)$

**贝叶斯决策**

贝叶斯决策（通常用$δ^∗$来表示）满足：

​									$ρ(δ^∗)= inf_δρ(δ)$

换句话说，贝叶斯决策$δ^∗$是在某个先验分布下使得平均风险最小的决策

寻找一般意义下的贝叶斯决策是相当不平凡的数学问题，为简洁、我们需要结合具体的机器学习算法来推导相应的贝叶斯决策。

## 2.参数估计

无论是贝叶斯学派还是频率学派，一个无法避开的问题就是**如何从已有的样本中获取信息并据此估计目标模型的参数**。比较有名的**“频率近似概率”**其实就是（基于大数定律的）相当合理的估计之一，本章所叙述的两种参数估计方法在最后也通常会归结于它.

### 极大似然估计(ML 估计)

如果把模型描述成一个概率模型的话，一个自然的想法是希望得到的模型参数 $θ$ 能够使得在训练集 $\breve{X}$ 作为输入时、模型输出的概率达到极大。这里就有一个似然函数的概念，它能够输出 $\breve{X}=(x_1,…,x_N)^T$ 在模型参数为 $θ$ 下的概率：

​									$p(\breve{X}|\theta)=\prod_{i=1}^n p(x_i|\theta)$					(1)

我们希望找到的 $\hat{\theta}$ 就是使得似然函数在X~作为输入时达到极大的参数：

​							$\hat{θ}=argmax_θ p(\breve{X}|θ)=argmax_θ\prod_{i=1}^Np(x_i|θ)$		(2)

举个栗子：假设一个暗箱中有白球、黑球共两个，虽然不知道具体的颜色分布情况、但是知道这两个球是完全一样的。现在有放回地从箱子里抽了 2 个球，发现两次抽出来的结果是 1 黑 1 白，那么该如何估计箱子里面球的颜色？从直观上来说似乎箱子中也是 1 黑 1 白会比较合理，下面我们就来说明“1 黑 1 白”这个估计就是极大似然估计。

在这个问题中，模型的参数θ可以设为从暗箱中抽出黑球的概率，样本xi可以描述为第i次取出的球是否是黑球；如果是就取 1、否则取 0。这样的话，似然函数就可以描述为：

​								$p(\breve{X}|θ)=θ^{x_1+x_2}(1−θ)^{2−x_1−x_2}$				(3)

直接对它求极大值（虽然可行但是）不太方便，通常的做法是将似然函数取对数之后再进行极大值的求解:

​		$In[p(\breve{X}|θ)]=(x_1+x_2)lnθ+(2−x_1−x_2)ln(1−θ)⇒\frac{∂lnp}{∂θ}=\frac{x_1+x_2}{θ}−\frac{2−x_1−x_2}{1−θ}$	(4)

从而可知：

​									$\frac{∂lnp}{∂θ}=0⇒θ=\frac{x_1+x_2}2$					(5)

由于$x_1+x_2=1$，所以得$\hat{θ}=0.5$、亦即应该估计从暗箱中抽出黑球的概率是 $50\%$；进一步地、既然暗箱中的两个球完全一样，我们应该估计暗箱中的颜色分布为 1 黑 1 白。

从以上的讨论可以看出，极大似然估计视待估参数为一个未知但固定的量、不考虑“观察者”的影响（亦即不考虑先验知识的影响），是传统的频率学派的做法

### 极大后验概率估计(MAP估计)

相比起极大似然估计，极大后验概率估计是更贴合贝叶斯学派思想的做法；事实上、甚至也有不少人直接称其为“贝叶斯估计”（注：贝叶斯估计的定义有许多，本人接触到的就有 3、4 种；囿于实力，本人无法辨析哪种才是真正的贝叶斯估计、所以我们不会进行相关的讨论）

在讨论 MAP 估计之前，我们有必要先知道何为后验概率$p(θ|\breve{X})$：它可以理解为参数 $θ$ 在训练集$\breve{X}$ 下所谓的“真实的出现概率”，能够利用参数的先验概率$p(θ)$、样本的先验概率$p(\breve{X})$和条件概率$p(\breve{X}|θ)=\prod_{i=1}^Np(x_i|θ)$通过贝叶斯公式导出

而 MAP 估计的核心思想、就是将待估参数$θ$看成是一个随机变量、从而引入了极大似然估计里面没有引入的、参数θ的先验分布。MAP 估计$\hat{θ}_{MAP}$的定义为：

​						$\hat{θ}_{MAP}=argmax_θp(θ|\breve{X})=argmax_θp(θ)\prod_{i=1}^Np(x_i|θ)$		(6)

同样的，为了计算简洁，我们通常对上式取对数：

​					$\hat{θ}_{MAP}=argmax_θlnp(θ|\breve{X})=argmax_θ[lnp(θ)+∑_{i=1}^{N}lnp(x_i|θ)]$	(7) 

可以看到，从形式上、极大后验概率估计只比极大似然估计多了lnp(θ)这一项，不过它们背后的思想却相当不同。不过有意思的是，在之后具体讨论朴素贝叶斯算法时我们会看到、朴素贝叶斯在估计参数时选用了极大似然估计法、但是在做决策时则选用了 MAP 估计

和极大似然估计相比，MAP 估计的一个显著优势在于它可以引入所谓的“先验知识”，这正是贝叶斯学派的精髓。当然这个优势同时也伴随着劣势：它我们对模型参数有相对较好的认知、否则会相当大地影响到结果的合理性

既然先验分布如此重要，那么是否有比较合理的、先验分布的选取方法呢？事实上，如何确定先验分布这个问题，正是贝叶斯统计中最困难、最具有争议性却又必须解决的问题。虽然这个问题确实有许多现代的研究成果，但遗憾的是，尚未能有一个圆满的理论和普适的方法。这里拟介绍“协调性假说”这个相对而言拥有比较好的直观的理论：

- 我们选择的参数θ的先验分布、应该与由它和训练集确定的后验分布属同一类型

此时先验分布又叫共轭先验分布。这里面所谓的“同一类型”其实又是难有恰当定义的概念，但是我们可以直观地理解为：概率性质相似的所有分布归为“同一类型”。比如，所有的正态分布都是“同一类型”的

## 3.朴素贝叶斯

### 算法过程：

首先要叙述朴素贝叶斯算法的基本假设：

- **独立性假设**：假设单一样本$X_i$的 n 个维度$X^{(1)}_i,...,X^{(n)}_i$彼此之间在各种意义上相互独立

这当然是很强的假设，在现实任务中也大多无法满足该假设。由此会衍生出所谓的半朴素贝叶斯和贝叶斯网，这里先按下不表

然后就是算法。我们打算先只叙述它的基本思想和各个公式，相关的定义和证明会放在后面的文章中。不过其实仅对着接下来的公式敲代码的话、就已经可以实现一个朴素贝叶斯模型了：

- 基本思想：

  - (1)对于给定训练数据，基于特征条件独立假设学习输入/输出的联合概率分布
  - (2)基于此模型，对给定的输入 $x$ ,利用贝叶斯定理求出后验概率最大的输出y

- 各个公式（假设输入有 $N$ 个、单个样本是 n 维的、一共有 $K$ 类：$c_1,...,c_K$）

  - 计算先验概率的极大似然估计：  

    ​			$\hat{p}(y=c_k)=\frac{∑^N_{i=1}I(y_i=c_k)}{N},k=1,2,...,K$					(8)

  - 计算条件概率的极大似然估计：

    ​			$\hat{p}(x^{(j)}=a_{jl}|y=c_k)=\frac{∑^{N}_{i=1}I(x^{(j)}_i=a_{jl},y_i=c_k)}{∑^N_{i=1}I(y_i=c_k)}$					(9)

    其中样本$x_i$第 $j$ 维 $x^{(j)}_i$ 的取值集合为$\{a_{j1},...,a_{jS_j}\}$ 

  - 得到最终的分类器:

    ​			$y=f(x^∗)=argmax_{c_k}\hat{p}(y=c_k)∏_{i=1}^n\hat{p}(x^{(i)}=x^{∗(i)}|y=c_k)$		(10)



在朴素贝叶斯算法思想下、一般来说会衍生出以下三种不同的模型：

- 离散型朴素贝叶斯（MultinomialNB）：所有维度的特征都是离散型随机变量
- 连续型朴素贝叶斯（GaussianNB）：所有维度的特征都是连续型随机变量
- 混合型朴素贝叶斯（MergedNB）：各个维度的特征有离散型也有连续型

接下来就简单~~（并不简单啊喂）~~讲讲朴素贝叶斯的数学背景。由浅入深，我们会用离散型朴素贝叶斯来说明一些普适性的概念，连续型和混合型的相关定义是类似的

### 朴素贝叶斯与贝叶斯决策论的联系：

朴素贝叶斯的模型参数即是类别的选择空间：

​					$\breve{Θ}=\{y=c_1,y=c_2,…,y=c_K\}$							(11)

朴素贝叶斯总的参数空间 $\breveΘ$ 本应包括模型参数的先验概率 $p(θ_k)=p(y=c_k)$、样本空间在模型参数下的条件概率 $p(X|θ_k)=p(X|y=c_k)$ 和样本空间本身的概率 $p(X)$；但由于我们采取样本空间的子集 $\breve{X}$ 作为训练集，所以在给定的 $\breve{X}$ 下、$p(X)=p(\breve{X})$ 是常数，因此可以把它从参数空间中删去。换句话说，我们关心的东西只有模型参数的先验概率和样本空间在模型参数下的条件概率

​					$\breve{Θ}=\{p(θ),p(X|θ):θ∈Θ\}$								(12)

行动空间 $A$ 就是朴素贝叶斯总的参数空间 $\breve{Θ}$

决策就是后验概率最大化:

​					$δ(\breve{X})=\hat{θ}=argmax_{\breve{θ}∈\breve{Θ}}p(\breve{θ}|\breve{X})$							(13)

在 $\hat{θ}$确定后，模型的决策就可以具体写成（这一步用到了独立性假设）

$f(x^∗)=argmax_{c_k}\hat{p}(c_k|X=x^∗)=argmax_{ck}\hat{p}(y=c_k)∏_{j=1}^n\hat{p}(X^{(j)}=x^{∗(j)}|y=c_k)$		(14)

损失函数会随模型的不同而不同。在离散型朴素贝叶斯中，损失函数就是比较简单的 0-1 损失函数:

​				$L(θ,δ(\breve{X}))=∑_{i=1}^N\breve{L}(y_i,f(x_i))=∑_{i=1}^NI(y_i≠f(x_i))$				(15)



这里的 $I$ 是示性函数，它满足：

​				$I(y_i\neq f(x_i))\left\{\begin{array}{cc} 1, &y_i \neq f(x_i)\\ 0, & y_i=f(x_i) \end{array}\right.$								(16)

从上述定义出发、可以利用[上一篇文章](http://www.carefree0910.com/posts/d007d6bc/)中讲解的两种参数估计方法导出离散型朴素贝叶斯的算法:

1.**输入**：训练数据集 $D={(x_1,y_1),…,(x_N,y_N)}$

2.**过程**（利用 ML 估计导出模型的具体参数）：

- 1.计算先验概率 $p(y=c_k)$ 的极大似然估计:

​				$\hat{p}(y=c_k)=\frac{∑^N_{i=1}I(y_i=c_k)}N, k=1,2,…,K$						(17)

- 2.计算条件概率$p(X^{(j)}=a_{jl}|y=c_k)$ 的极大似然估计（设每一个单独输入的 $n$ 维向量 $x_i$的第 			  $j$ 维特征 $x^{(j)}$ 可能的取值集合为$\{a_{j1},…,a_{jS_j}\}$:

​				  $\hat{p}(X^{(j)}=a_{jl}|y=c_k)=\frac{∑^N_{i=1}I(x^{(j)}_i=a_{jl},y_i=c_k)}{∑^N_{i=1}I(y_i=c_k)}$						(18)

3.**输出**(利用 MAP 估计进行决策)朴素贝叶斯模型，能够估计数据$x^∗=(x^{∗(1)},…,x^{∗(n)})^T$ 的类别：  

​				$y=f(x^∗)=argmax_{c_k}\hat{p}(y=c_k)∏_{j=1}^n\hat{p}(X^{(j)}=x^{∗(j)}|y=c_k)$		(19)



由上述算法可以清晰地梳理出朴素贝叶斯算法背后的数学思想：

- 使用极大似然估计导出模型的具体参数（先验概率、条件概率）
- 使用极大后验概率估计作为模型的决策（输出使得数据后验概率最大化的类别）